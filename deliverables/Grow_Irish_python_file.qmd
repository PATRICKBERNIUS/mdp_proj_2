---
title: "Initial Exploratory Analysis"
format: html
---


```{python}
#importing packages
import glob
import pandas as pd
from tqdm import tqdm
import numpy as np
import re
import plotly.express as px
```


```{python}

#reading files
files = glob.glob(r"G:\Shared drives\Grow Irish Week/Datasets/*")


#df for each csv
dfs = [] 

#append each csv to list
for file in files:
    df = pd.read_csv(file)
    dfs.append(df)

#combining csvs
full_df = pd.concat(dfs)

```




```{python}
#cleaning

#splitting file name
full_df[['date', 'player_id', 'event']] = full_df['file'].str.split(r"/", expand=True)

#getting raw playerID
full_df['player_id'] = full_df['player_id'].str.replace(r'player_', "", regex=True)

#cleaning event column
full_df['event'] = full_df['event'].str.replace(r'.json.gz', "", regex=True)

#converting date to datetime
full_df['date'] = pd.to_datetime(full_df['date'])

#dropping altitude column
full_df = full_df.drop(['altitude'], axis=1)
full_df
```

```{python}
#saving to csv
full_df.to_csv('full_players_df.csv', index=False)
```



```{python}
full_df = pd.read_csv('full_players_df.csv')
full_df['date'] = pd.to_datetime(full_df['date'])
full_df = full_df.drop(columns=['Unnamed: 0'])
full_df
```

```{python}
full_df

full_df['player_number'] = pd.factorize(full_df['player_id'])[0]+1
```


```{python}
#exploring numerical variables


columns_to_plot = ['speed', 'odometer', 'acc', 'hr', 'mp', 'cadence']

for column in columns_to_plot:
    fig = px.histogram(full_df, x=column)
    fig.show()

```



```{python}

reg_df = full_df.dropna(subset=columns_to_plot)

X = reg_df[['speed', 'odometer', 'acc', 'mp', 'cadence']]

y = reg_df['hr']



```



```{python}
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, log_loss, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score, r2_score
)

X = reg_df[['speed', 'odometer', 'acc', 'mp', 'cadence']]

y = reg_df['hr']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

hr_model = xgb.XGBRegressor(
    objective="reg:squarederror",
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    random_state=42
)


hr_model.fit(X_train, y_train)


y_pred = hr_model.predict(X_test)
#y_pred_prob = hr_model.predict_proba(X_test)


print(r2_score(y_test, y_pred))

#Visualizing Feature Importance

import seaborn as sns
import matplotlib.pyplot as plt

# Feature Importance
importances_sim = hr_model.feature_importances_
feature_names_sim = X_train.columns

feat_imp_sim_df = pd.DataFrame({
    "Feature": feature_names_sim,
    "Importance": importances_sim
}).sort_values(by="Importance", ascending=False)


plt.figure(figsize=(8,5))
sns.barplot(x="Importance", y="Feature", data=feat_imp_sim_df, palette="viridis")
plt.title("Feature Importances - HR Prediction Model")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

print("\nFeature Importance Rankings:")
print(feat_imp_sim_df)
```





```{python}
X = reg_df[['speed', 'odometer', 'acc', 'hr', 'cadence']]

y = reg_df['mp']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

hr_model = xgb.XGBRegressor(
    objective="reg:squarederror",
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    random_state=42
)


hr_model.fit(X_train, y_train)


y_pred = hr_model.predict(X_test)
#y_pred_prob = hr_model.predict_proba(X_test)


print(r2_score(y_test, y_pred))


# Feature Importance
importances_sim = hr_model.feature_importances_
feature_names_sim = X_train.columns

feat_imp_sim_df = pd.DataFrame({
    "Feature": feature_names_sim,
    "Importance": importances_sim
}).sort_values(by="Importance", ascending=False)


plt.figure(figsize=(8,5))
sns.barplot(x="Importance", y="Feature", data=feat_imp_sim_df, palette="viridis")
plt.title("Feature Importances - HR Prediction Model")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

print("\nFeature Importance Rankings:")
print(feat_imp_sim_df)
```










```{python}
#scaling numerical values

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = MinMaxScaler()

#dropping rows where heart rate is null
hr_df = full_df.dropna(subset=['hr']).copy()

scaled_values = scaler.fit_transform((hr_df[columns_to_plot]))

#converting to dataframe
scaled_df = pd.DataFrame(
    scaled_values,
    columns=columns_to_plot,
    index=hr_df.index
)



```



```{python}
#assigning weights to variables
w_speed = 0.4
w_acc = 0.3
w_hr = 0.1
w_mp = 0.2

#calculating intensity
scaled_df['intensity'] = (
    w_speed * scaled_df['speed'] + 
    w_acc * scaled_df['acc'] + 
    w_hr * scaled_df['hr'] + 
    w_mp * scaled_df['mp']
)



scaled_cols = scaled_df[columns_to_plot]

#denormalizing scaled values
denormalized_scaled = scaler.inverse_transform(scaled_cols)

denormalized_df = scaled_df.copy()

#converting back to df
denormalized_df[columns_to_plot] = denormalized_scaled

#adding intensity score
denormalized_df['intensity'] = scaled_df['intensity']
```







```{python}
#adding intensity values to original df
hr_df['intensity'] = denormalized_df['intensity']

#hr_df.to_csv('hr_df.csv', index=False)

```



```{python}
#grouping by max intensity for player
grouped_player = hr_df.groupby('player_id')['intensity'].max()
```



```{python}

#converting timestamp to measurable date
hr_df['timestamp'] = pd.to_datetime(hr_df['timestamp'], unit='ms')
```




```{python}


time_df.to_csv('time_stamped_df.csv')
```



```{python}
#ensuring datetime dtype
hr_df['timestamp'] = pd.to_datetime(hr_df['timestamp'])

#sorting by player and time
hr_df = hr_df.sort_values(['player_id', 'timestamp']).reset_index(drop=True)

#5 second rolling window
roll5 = (
    hr_df.groupby('player_id')
         .rolling('5s', on='timestamp')['intensity']
         .mean()
)

#10 second rolling window
roll10 = (
    hr_df.groupby('player_id')
         .rolling('10s', on='timestamp')['intensity']
         .mean()
)

#20 second rolling window
roll20 = (
    hr_df.groupby('player_id')
         .rolling('20s', on='timestamp')['intensity']
         .mean()
)

#30 second rolling window
roll30 = (
    hr_df.groupby('player_id')
         .rolling('30s', on='timestamp')['intensity']
         .mean()
)

#converting to numpy array and adding back to df
hr_df['intensity_5s'] = roll5.to_numpy()
hr_df['intensity_10s'] = roll10.to_numpy()
hr_df['intensity_20s'] = roll20.to_numpy()
hr_df['intensity_30s'] = roll30.to_numpy()

```


```{python}
#grouping by player and max exertion for each window
grouped_df = hr_df.groupby('player_id')[['intensity_5s', 'intensity_10s', 'intensity_20s', 'intensity_30s']].max()

grouped_df.sort_values(by='intensity_5s', ascending=False)
```


```{python}

#splitting df by session/date
session_dates = hr_df['date'].unique()[:4]
session_dfs = []

#looping through dates
for d in session_dates:
    df_session = hr_df[hr_df['date'] == d].copy()
    session_dfs.append(df_session)

first_sesh = session_dfs[0]
second_sesh = session_dfs[1]
third_sesh = session_dfs[2]
fourth_sesh = session_dfs[3]

```




```{python}
import matplotlib.pyplot as plt

player_id = first_session_df['player_id'].unique()[1]
p = first_session_df[first_session_df['player_id'] == player_id]

plt.figure(figsize=(14,5))

plt.plot(p['timestamp'], p['intensity'], alpha=0.3, label="Instant intensity")
plt.plot(p['timestamp'], p['intensity_10s'], label="10s rolling", linewidth=2)
plt.plot(p['timestamp'], p['intensity_20s'], label="20s rolling")

plt.title(f"Training Session {session_date} – Player {player_id}")
plt.xlabel("Time")
plt.ylabel("Intensity")
plt.legend()
plt.tight_layout()
plt.show()
```



```{python}
import plotly.express as px

player_id = first_session_df['player_id'].unique()[0]
p = first_session_df[first_session_df['player_id'] == player_id].copy()

plot_df = p.melt(
    id_vars='timestamp',
    value_vars=['intensity', 'intensity_10s', 'intensity_20s'],
    var_name='series',
    value_name='value'
)

fig = px.line(
    plot_df,
    x='timestamp',
    y='value',
    color='series',
    title=f"Training Session {session_date} – Player {player_id}",
    labels={'timestamp': 'Time', 'value': 'Intensity', 'series': 'Series'}
)

fig.update_traces(
    selector=lambda t: t.name == 'intensity',
    opacity=0.3
)

fig.show()
```





```{python}
sesh = first_sesh.copy()

avg_sesh = (
    sesh.groupby('timestamp')[['intensity', 'intensity_5s', 'intensity_10s', 'intensity_20s']].mean()
    .reset_index()
)

#melting for plotly
plot_df = (
    avg_sesh.melt(
        id_vars='timestamp',
        value_vars=['intensity', 'intensity_5s', 'intensity_10s', 'intensity_20s'],
        var_name='series',
        value_name='value'
    )
)



fig = px.line(
    plot_df,
    x='timestamp',
    y='value',
    color='series'
)

fig.show()
```



```{python}
first_player = first_sesh['player_id'].unique()[0]
first_player_df = first_sesh[first_sesh['player_id'] == first_player].copy()
first_player_df


first_player_df.to_csv('first_player_df.csv')


```